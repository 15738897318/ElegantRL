{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reacher_Example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI4Finance-LLC/ElegantRL/blob/master/Reacher_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1gUG3OCJ5GS"
      },
      "source": [
        "# **Reacher-v2 Example in ElegantRL**\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGXyBBvL0dR2"
      },
      "source": [
        "# **Part 1: Problem Definition**\r\n",
        "\r\n",
        "The goal of the [Reacher-v2](https://gym.openai.com/envs/Reacher-v2/) task is straightforward: we want to make a 2D robot reach to a randomly located target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbamGVHC3AeW"
      },
      "source": [
        "# **Part 2: Install all the packages through ElegantRL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U35bhkUqOqbS",
        "outputId": "e7be2eb0-c097-4e70-bb58-f655c1632395"
      },
      "source": [
        "# install elegantrl library\n",
        "!pip install git+https://github.com/AI4Finance-LLC/ElegantRL.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/AI4Finance-LLC/ElegantRL.git\n",
            "  Cloning https://github.com/AI4Finance-LLC/ElegantRL.git to /tmp/pip-req-build-6febpv_h\n",
            "  Running command git clone -q https://github.com/AI4Finance-LLC/ElegantRL.git /tmp/pip-req-build-6febpv_h\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.1) (0.17.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.1) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.1) (1.19.5)\n",
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/b6/719c6e1741fe6126c99d9f3a96fbb9f024ec12a60e6718843f33c7cab1b0/pybullet-3.0.8-cp37-cp37m-manylinux1_x86_64.whl (76.6MB)\n",
            "\u001b[K     |████████████████████████████████| 76.6MB 69kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.1) (1.7.1+cu101)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.1) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->elegantrl==0.3.1) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->elegantrl==0.3.1) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->elegantrl==0.3.1) (1.5.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.1) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.1) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.1) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.1) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->elegantrl==0.3.1) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->elegantrl==0.3.1) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->elegantrl==0.3.1) (1.15.0)\n",
            "Building wheels for collected packages: elegantrl\n",
            "  Building wheel for elegantrl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for elegantrl: filename=elegantrl-0.3.1-cp37-none-any.whl size=75050 sha256=7d646391383a02fe96e55c50d0aa328d226d5e4bcc2cf17589bc9353498c6eee\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fd0upk1_/wheels/d0/f4/2e/cec0c14b57c2094a2bcef3063f95d758ad1309a640ff100419\n",
            "Successfully built elegantrl\n",
            "Installing collected packages: pybullet, elegantrl\n",
            "Successfully installed elegantrl-0.3.1 pybullet-3.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVdmpnK_3Zcn"
      },
      "source": [
        "# **Part 3: Import packages**\r\n",
        "\r\n",
        "\r\n",
        "*   elegantrl\r\n",
        "*   OpenAI gym\r\n",
        "*   PyBullet gym\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VM1xKujoz-6"
      },
      "source": [
        "from elegantrl.run import *\r\n",
        "from elegantrl.env import prep_env\r\n",
        "import elegantrl.agent as agent\r\n",
        "import gym\r\n",
        "import pybullet_envs  # for python-bullet-gym\r\n",
        "dir(pybullet_envs)\r\n",
        "gym.logger.set_level(40) # Block warning"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n8zcgcn14uq"
      },
      "source": [
        "# **Part 4: Initialize agent and environment**\r\n",
        "\r\n",
        "*   **break_step**: the maximum training steps if the target reward is not reached.\r\n",
        "*  **eval_times1**: the evaluation times if 'eval_reward > old_max_reward'.\r\n",
        "*   **eval_times2**: the evaluation times if 'eval_reward > target_reward'.\r\n",
        "*   **rollout_num**: the number of rollout workers (larger is not always faster).\r\n",
        "\r\n",
        "\r\n",
        "> See Arguments() for more details about adjustable hyper-parameters, and user is able to import the customized environment for own task.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E03f6cTeajK4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e88ad249-7f06-4e1b-a7f4-7171e3bdd9b8"
      },
      "source": [
        "args = Arguments(if_on_policy=True)\r\n",
        "args.agent_rl = agent.AgentGaePPO  # choose an DRL algorithm\r\n",
        "args.env = prep_env(gym.make('ReacherBulletEnv-v0')) # create and preprocess the environment from gym\r\n",
        "\r\n",
        "args.break_step = int(5e4 * 8)  # (5e4) 1e5, UsedTime: (300s) 800s\r\n",
        "args.eval_times1 = 1\r\n",
        "args.eval_times2 = 2\r\n",
        "args.rollout_num = 4"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| env_name: ReacherBulletEnv-v0, action space: Continuous\n",
            "| state_dim: 9, action_dim: 2, action_max: 1.0, target_reward: 18.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1j5kLHF2dhJ"
      },
      "source": [
        "# **Part 5: Train and evaluate the model**\r\n",
        "\r\n",
        "> In order to train and evaluate the model, the user only needs to pass the Arguments into the function, and the model would be automatically generated. In this case, all work the user needs to do is to define a proper environment for the agent to interact with.\r\n",
        "\r\n",
        "\r\n",
        "*   **Step**: the total training steps.\r\n",
        "*  **MaxR**: the maximum reward.\r\n",
        "*   **avgR**: the average of the rewards.\r\n",
        "*   **stdR**: the standard deviation of the rewards.\r\n",
        "*   **objA**: the objective function value of Actor Network (Policy Network).\r\n",
        "*   **objC**: the objective function value (Q-value)  of Critic Network (Value Network).\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGOPSD6da23k",
        "outputId": "7ae2951b-0bc4-4730-aa5a-c994a887891a"
      },
      "source": [
        "train_and_evaluate__multiprocessing(args) # the training process will terminate once it reaches the target reward."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| GPU id: 0, cwd: ./AgentGaePPO/ReacherBulletEnv-v0_0\n",
            "| Remove history\n",
            "ID      Step      MaxR |    avgR      stdR       objA      objC\n",
            "0   0.00e+00     -8.48 |\n",
            "0   1.05e+03     14.32 |\n",
            "0   9.45e+03     14.32 |   -2.92      0.00      -0.56      2.65\n",
            "0   1.78e+04     14.32 |    1.95      0.00      -0.62      1.66\n",
            "0   2.52e+04     20.50 |\n",
            "ID      Step   TargetR |    avgR      stdR   UsedTime  ########\n",
            "0   2.62e+04     18.00 |   20.50      3.78        808  ########\n",
            "0   2.62e+04     20.50 |   20.50      3.78      -0.67      1.02\n",
            "| print_norm: state_avg, state_fix_std\n",
            "| avg = np.array([ 0.22324559, -0.8252825 , -0.13812736,  0.65398854, -0.03838773,\n",
            "        0.19503878,  0.8229074 , -0.18179317, -0.10579718], dtype=np.float32)\n",
            "| std = np.array([0.11132354, 0.12903091, 0.13470675, 0.15849867, 0.518055  ,\n",
            "       0.51017815, 0.2939634 , 0.55595165, 0.54980254], dtype=np.float32)\n",
            "| SavedDir: ./AgentGaePPO/ReacherBulletEnv-v0_0\n",
            "| UsedTime: 808\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}