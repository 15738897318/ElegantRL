{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: Discrete action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm\n",
    "from AgentZoo import AgentDoubleDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "from AgentRun import Arguments, train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| env_name: CartPole-v0, action space: if_discrete\n",
      "| state_dim: 4, action_dim: 2, action_max: 1, target_reward: 195.0\n"
     ]
    }
   ],
   "source": [
    "# environment\n",
    "from Env import decorate_env\n",
    "import gym  # gym of OpenAI is not necessary for ElegantRL (even RL)\n",
    "gym.logger.set_level(40)  # Block warning: 'WARN: Box bound precision lowered by casting to float32'\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env = decorate_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| GPU id: 0, cwd: ./AgentDoubleDQN/CartPole-v0_0\n",
      "| Remove history\n",
      "ID      Step      MaxR |    avgR      stdR       objA      objC\n",
      "0   0.00e+00    200.00 |\n",
      "ID      Step   TargetR |    avgR      stdR   UsedTime  ########\n",
      "0   1.02e+03    195.00 |  200.00      0.00         13  ########\n"
     ]
    }
   ],
   "source": [
    "# class for setting and hyper-parameters\n",
    "args = Arguments(agent_rl=AgentDoubleDQN, env=env, gpu_id=0)\n",
    "\n",
    "# set hyper-parameters\n",
    "args.break_step = int(1e3 * 8)  # UsedTime: 20s (reach target_reward 195)\n",
    "args.net_dim = 2 ** 7\n",
    "\n",
    "# train the agent\n",
    "train_and_evaluate(args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| env_name: LunarLander-v2, action space: if_discrete\n",
      "| state_dim: 8, action_dim: 4, action_max: 1, target_reward: 200\n",
      "| GPU id: 0, cwd: ./AgentDoubleDQN/CartPole-v0_0\n",
      "| Remove history\n",
      "ID      Step      MaxR |    avgR      stdR       objA      objC\n",
      "0   0.00e+00   -229.44 |\n",
      "0   1.02e+03   -157.52 |\n",
      "0   5.12e+03   -114.14 |\n",
      "0   6.14e+03    -51.93 |\n",
      "0   8.19e+03    106.39 |\n",
      "0   1.33e+04    215.38 |\n",
      "ID      Step   TargetR |    avgR      stdR   UsedTime  ########\n",
      "0   1.43e+04    200.00 |  215.38    103.27        194  ########\n"
     ]
    }
   ],
   "source": [
    "# train the agent in another env\n",
    "args.env = decorate_env(env=gym.make('LunarLander-v2'))\n",
    "args.net_dim = 2 ** 8  # change a default hyper-parameters\n",
    "args.break_step = int(1e5 * 8)  # UsedTime: 200s (reach target_reward 200)\n",
    "\n",
    "train_and_evaluate(args) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Continuous action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym  # don't worry about 'WARN: Box bound precision lowered by casting to float32'\n",
    "from AgentRun import Arguments, train_and_evaluate\n",
    "from AgentZoo import AgentTD3, AgentSAC, AgentPPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2.1: Off-policy TD3 and SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| env.spec.reward_threshold is None, so I set target_reward=-200\n",
      "| env_name: Pendulum-v0, action space: Continuous\n",
      "| state_dim: 3, action_dim: 1, action_max: 2.0, target_reward: -200\n",
      "| GPU id: j, cwd: ./AgentSAC/Pendulum-v0_j\n",
      "| Remove history\n",
      "ID      Step      MaxR |    avgR      stdR       objA      objC\n",
      "j   0.00e+00  -1340.63 |\n",
      "j   1.02e+03   -970.71 |\n",
      "j   4.10e+03   -817.50 |\n",
      "j   5.12e+03   -357.35 |\n",
      "j   6.14e+03   -259.11 |\n",
      "j   7.17e+03   -228.45 |\n",
      "j   8.19e+03   -158.32 |\n",
      "ID      Step   TargetR |    avgR      stdR   UsedTime  ########\n",
      "j   9.22e+03   -200.00 | -158.32    131.10        155  ########\n"
     ]
    }
   ],
   "source": [
    "args = Arguments(if_on_policy=False)  # if_on_policy=False in default\n",
    "\n",
    "# choose DRL algorithm (off-policy)\n",
    "args.agent_rl = AgentSAC  # AgentTD3\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.target_reward = -200  # set target_reward manually for env 'Pendulum-v0'\n",
    "args.env = decorate_env(env)\n",
    "args.reward_scale = 2 ** -3\n",
    "args.break_step = int(1e4 * 8)  # UsedTime: 200s (reach target_reward -200)\n",
    "# args.env = decorate_env(env=gym.make('LunarLanderContinuous-v2'))\n",
    "# args.reward_scale = 2 ** -2\n",
    "# args.env = decorate_env(env=gym.make('BipedalWalker-v3'))  # recommend args.gamma = 0.95\n",
    "# args.gamma = 0.96\n",
    "\n",
    "train_and_evaluate(args) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2.2: On-policy PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| env.spec.reward_threshold is None, so I set target_reward=-200\n",
      "| env_name: Pendulum-v0, action space: Continuous\n",
      "| state_dim: 3, action_dim: 1, action_max: 2.0, target_reward: -200\n",
      "| GPU id: j, cwd: ./AgentPPO/Pendulum-v0_j\n",
      "| Remove history\n",
      "ID      Step      MaxR |    avgR      stdR       objA      objC\n",
      "j   0.00e+00  -1375.25 |\n",
      "j   9.60e+03  -1216.81 |\n",
      "j   1.28e+04  -1196.24 |\n",
      "j   1.60e+04  -1129.99 |\n",
      "j   4.80e+04  -1051.96 |\n",
      "j   8.32e+04   -999.29 |\n",
      "j   8.64e+04   -876.76 |\n",
      "j   9.92e+04   -797.66 |\n",
      "j   1.06e+05   -737.22 |\n",
      "j   1.12e+05   -732.56 |\n",
      "j   1.15e+05   -673.28 |\n",
      "j   1.22e+05   -649.79 |\n",
      "j   1.28e+05   -606.30 |\n",
      "j   1.73e+05   -562.12 |\n",
      "j   1.76e+05   -559.34 |\n",
      "j   1.79e+05   -552.79 |\n",
      "j   1.82e+05   -468.47 |\n",
      "j   1.86e+05   -401.66 |\n",
      "j   1.89e+05   -153.41 |\n",
      "ID      Step   TargetR |    avgR      stdR   UsedTime  ########\n",
      "j   1.92e+05   -200.00 | -153.41    101.49        228  ########\n"
     ]
    }
   ],
   "source": [
    "args = Arguments(if_on_policy=True)  # on-policy has different hyper-parameters from off-policy\n",
    "\n",
    "# choose DRL algorithm (off-policy)\n",
    "args.agent_rl = AgentPPO\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.target_reward = -200  # set target_reward manually for env 'Pendulum-v0'\n",
    "args.env = decorate_env(env)\n",
    "args.break_step = int(2e5 * 8)  # UsedTime: 300s (reach target_reward -200)\n",
    "args.reward_scale = 2 ** -3\n",
    "# args.env = decorate_env(env=gym.make('LunarLanderContinuous-v2'))\n",
    "# args.reward_scale = 2 ** -2\n",
    "# args.env = decorate_env(env=gym.make('BipedalWalker-v3'))  # recommend args.gamma = 0.95\n",
    "# args.gamma = 0.96\n",
    "\n",
    "train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 3: FinanceMultiStockEnv from FinRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AgentEnv import FinanceMultiStockEnv\n",
    "from AgentRun import Arguments, train_and_evaluate\n",
    "from AgentZoo import AgentTD3, AgentSAC, AgentPPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FinanceMultiStockEnv()  # a standard env for ElegantRL, not need decorate_env()\n",
    "env.target_reward = 5\n",
    "\n",
    "args = Arguments(if_on_policy=True)\n",
    "args.agent_rl = AgentPPO  # 600s\n",
    "args.env = env\n",
    "args.eval_times = 1\n",
    "args.break_step = int(1e6 * 8)  # UsedTime: 1000s (reach target_reward 5)\n",
    "\n",
    "args.break_step = int(5e6 * 4) \n",
    "args.net_dim = 2 ** 8\n",
    "args.max_step = 1699\n",
    "args.max_memo = (args.max_step - 1) * 8\n",
    "args.batch_size = 2 ** 11\n",
    "args.repeat_times = 2 ** 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| GPU id: j, cwd: ./AgentPPO/FinanceStock-v1_j\n",
      "| Remove history\n",
      "ID      Step      MaxR |    avgR      stdR       objA      objC\n",
      "j   0.00e+00      2.03 |\n",
      "j   3.57e+04      2.18 |\n",
      "j   4.75e+04      2.32 |\n",
      "j   7.13e+04      2.40 |\n",
      "j   9.51e+04      2.57 |\n",
      "j   1.19e+05      2.63 |\n",
      "j   1.31e+05      2.69 |\n",
      "j   1.78e+05      2.72 |\n",
      "j   2.38e+05      2.90 |\n",
      "j   2.61e+05      3.13 |\n",
      "j   2.97e+05      3.13 |    3.03      0.00      -0.04      0.03\n",
      "j   4.40e+05      3.37 |\n",
      "j   4.64e+05      3.43 |\n",
      "j   4.75e+05      3.56 |\n",
      "j   5.59e+05      3.85 |\n",
      "j   5.71e+05      3.86 |\n",
      "j   5.94e+05      3.86 |    3.67      0.00      -0.31      0.03\n",
      "j   6.06e+05      3.99 |\n",
      "j   6.18e+05      4.30 |\n",
      "j   6.30e+05      4.35 |\n",
      "j   6.42e+05      4.44 |\n",
      "j   6.54e+05      4.56 |\n",
      "j   7.01e+05      4.63 |\n",
      "j   7.37e+05      4.68 |\n",
      "j   7.61e+05      4.77 |\n",
      "j   7.96e+05      4.81 |\n",
      "j   8.56e+05      5.01 |\n",
      "ID      Step   TargetR |    avgR      stdR   UsedTime  ########\n",
      "j   8.68e+05      5.00 |    5.01      0.00        754  ########\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 4: PyBullet (MuJoCo) (wait for adding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym  # don't worry about 'WARN: Box bound precision lowered by casting to float32'\n",
    "import pybullet_envs  # PyBullet is free, but MuJoCo is paid\n",
    "from AgentEnv import decorate_env\n",
    "from AgentRun import Arguments, train_and_evaluate\n",
    "from AgentZoo import AgentTD3, AgentSAC, AgentPPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| env_name: AntBulletEnv-v0, action space: Continuous\n",
      "| state_dim: 28, action_dim: 8, action_max: 1.0, target_reward: 2500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb1/Yonv/conda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env_name = 'AntBulletEnv-v0'\n",
    "assert env_name in {\n",
    "    \"AntBulletEnv-v0\", \n",
    "    \"Walker2DBulletEnv-v0\", \n",
    "    \"HalfCheetahBulletEnv-v0\",\n",
    "    \"HumanoidBulletEnv-v0\", \n",
    "    \"HumanoidFlagrunBulletEnv-v0\", \n",
    "    \"HumanoidFlagrunHarderBulletEnv-v0\",\n",
    "}\n",
    "env = gym.make(env_name)\n",
    "env = decorate_env(env, if_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments()\n",
    "args.agent_rl = AgentSAC  # AgentSAC can't reach target_reward=2500, try AgentModSAC\n",
    "args.env = env\n",
    "args.reward_scale = 2 ** -3\n",
    "args.break_step = int(1e6 * 8)\n",
    "args.eval_times = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| GPU id: 0, cwd: ./AgentSAC/AntBulletEnv-v0_0\n",
      "| Remove history\n",
      "ID      Step      MaxR |    avgR      stdR       objA      objC\n",
      "0   0.00e+00    251.93 |\n",
      "0   1.02e+03    292.26 |\n",
      "0   3.07e+03    350.33 |\n",
      "0   5.12e+03    421.54 |\n",
      "0   6.14e+03    513.66 |\n",
      "0   1.33e+04    513.66 |  333.49      4.98       0.01      0.08\n",
      "0   2.76e+04    513.66 |  111.80     24.59       0.06      0.99\n",
      "0   4.20e+04    513.66 |   24.61      6.73       0.06      0.82\n",
      "0   4.51e+04    529.52 |\n",
      "0   5.12e+04    575.41 |\n",
      "0   5.63e+04    575.41 |  373.09     68.59       0.05      0.13\n",
      "0   6.96e+04    575.41 |  245.12     45.75       0.04      0.03\n",
      "0   8.29e+04    575.41 |  364.47      0.91       0.03      0.05\n",
      "0   9.63e+04    575.41 |  531.49      8.53       0.03      0.04\n",
      "0   9.83e+04    593.06 |\n",
      "0   9.93e+04    597.63 |\n",
      "0   1.08e+05    660.47 |\n",
      "0   1.10e+05    660.47 |  598.99     98.36       0.03      0.02\n",
      "0   1.23e+05    660.47 |  334.37    233.92       0.02      0.07\n",
      "0   1.36e+05    660.47 |  216.42    182.18       0.03      0.03\n",
      "0   1.50e+05    660.47 |  255.07     17.58       0.02      0.03\n",
      "0   1.64e+05    660.47 |  415.59      3.06       0.04      0.49\n",
      "0   1.66e+05    697.63 |\n",
      "0   1.78e+05    697.63 |  342.00     11.92       0.07      0.32\n",
      "0   1.93e+05    697.63 |  347.33     47.41       0.21      5.08\n",
      "0   2.07e+05    697.63 |  352.06     25.00       0.49      6.79\n",
      "0   2.21e+05    697.63 |  500.85      6.40       0.31      4.72\n",
      "0   2.36e+05    697.63 |  256.48    211.34       0.31      4.07\n",
      "0   2.49e+05    697.63 |  518.92     25.06       0.30      4.21\n",
      "0   2.62e+05    697.63 |  314.12    213.63       0.23      2.52\n",
      "0   2.78e+05    697.63 |   18.75      1.78       0.29      4.69\n",
      "0   2.93e+05    697.63 |   58.57      2.77       0.29      3.44\n",
      "0   3.08e+05    697.63 |  532.07      6.46       0.24      2.56\n",
      "0   3.24e+05    697.63 |  100.68     40.51       0.25      2.89\n",
      "0   3.39e+05    697.63 |  487.90    117.39       0.19      1.80\n",
      "0   3.53e+05    697.63 |  367.56    122.80       0.14      2.54\n",
      "0   3.66e+05    697.63 |  249.27    204.35       0.09      1.31\n",
      "0   3.79e+05    697.63 |  348.01     49.50       0.10      1.68\n",
      "0   3.93e+05    697.63 |   45.57      1.77       0.09      1.49\n",
      "0   4.07e+05    697.63 |  220.27    174.08       0.09      1.61\n",
      "0   4.21e+05    697.63 |  446.38    132.17       0.07      0.53\n",
      "0   4.31e+05    716.84 |\n",
      "0   4.34e+05    716.84 |  444.01     65.51       0.06      1.99\n",
      "0   4.50e+05    716.84 |   27.64      1.91       0.09      1.86\n",
      "0   4.65e+05    716.84 |   45.48      0.28       0.07      1.37\n",
      "0   4.80e+05    716.84 |   30.91      1.70       0.05      0.37\n",
      "0   4.94e+05    716.84 |  311.14    337.45       0.07      0.84\n",
      "0   5.06e+05    716.84 |  420.98     44.18       0.05      0.34\n",
      "0   5.08e+05    837.56 |\n",
      "0   5.20e+05    837.56 |  658.15     70.06       0.03      0.11\n",
      "0   5.35e+05    837.56 |  655.21      0.34       0.02      0.07\n",
      "0   5.43e+05    853.54 |\n",
      "0   5.49e+05    853.54 |  594.37     32.87       0.01      0.03\n",
      "0   5.62e+05    853.54 |  590.14    132.82       0.01      0.02\n",
      "0   5.68e+05    940.68 |\n",
      "0   5.75e+05    940.68 |  781.73    195.95       0.01      0.02\n",
      "0   5.89e+05    940.68 |  884.19     35.36       0.01      0.02\n",
      "0   6.02e+05    940.68 |  705.81      2.94       0.01      0.20\n",
      "0   6.15e+05    940.68 |  677.53     37.25       0.01      0.01\n",
      "0   6.16e+05   1084.03 |\n",
      "0   6.29e+05   1084.03 |  860.66     25.17       0.01      0.01\n",
      "0   6.39e+05   1149.78 |\n",
      "0   6.41e+05   1149.78 |  894.55     66.05       0.01      0.01\n",
      "0   6.42e+05   1228.14 |\n",
      "0   6.52e+05   1236.30 |\n",
      "0   6.54e+05   1307.89 |\n",
      "0   6.55e+05   1307.89 | 1307.89     67.28       0.01      0.01\n",
      "0   6.56e+05   1309.63 |\n",
      "0   6.59e+05   1398.27 |\n",
      "0   6.63e+05   1544.75 |\n",
      "0   6.69e+05   1544.75 | 1530.34     11.97       0.01      0.01\n",
      "0   6.72e+05   1593.23 |\n",
      "0   6.78e+05   1661.13 |\n",
      "0   6.82e+05   1661.13 | 1591.40     15.85       0.01      0.02\n",
      "0   6.84e+05   1753.67 |\n",
      "0   6.85e+05   1762.63 |\n",
      "0   6.95e+05   1762.63 | 1617.70     18.11       0.01      0.01\n",
      "0   6.98e+05   1764.50 |\n",
      "0   6.99e+05   1793.03 |\n",
      "0   7.02e+05   1816.41 |\n",
      "0   7.03e+05   1844.54 |\n",
      "0   7.05e+05   1862.06 |\n",
      "0   7.09e+05   1862.06 | 1192.86    651.92       0.01      0.02\n",
      "0   7.09e+05   1895.88 |\n",
      "0   7.22e+05   1895.88 | 1873.19      0.73       0.02      0.01\n",
      "0   7.26e+05   1916.85 |\n",
      "0   7.27e+05   1938.08 |\n",
      "0   7.28e+05   1954.35 |\n",
      "0   7.33e+05   1992.51 |\n",
      "0   7.35e+05   1992.51 | 1951.63      8.61       0.02      0.01\n",
      "0   7.36e+05   2010.23 |\n",
      "0   7.38e+05   2011.10 |\n",
      "0   7.44e+05   2034.00 |\n",
      "0   7.46e+05   2039.42 |\n",
      "0   7.49e+05   2039.42 | 1215.17    803.35       0.02      0.36\n",
      "0   7.49e+05   2050.37 |\n",
      "0   7.58e+05   2052.11 |\n",
      "0   7.59e+05   2122.73 |\n",
      "0   7.62e+05   2122.73 | 2107.89      0.03       0.02      0.01\n",
      "0   7.69e+05   2165.23 |\n",
      "0   7.75e+05   2165.23 | 2140.53      2.15       0.02      0.01\n",
      "0   7.76e+05   2178.07 |\n",
      "0   7.79e+05   2200.69 |\n",
      "0   7.81e+05   2204.23 |\n",
      "0   7.84e+05   2220.49 |\n",
      "0   7.86e+05   2228.05 |\n",
      "0   7.87e+05   2233.59 |\n",
      "0   7.88e+05   2233.59 | 2233.59      4.02       0.01      0.01\n",
      "0   8.03e+05   2233.59 |  606.55    114.44       0.01      0.42\n",
      "0   8.09e+05   2234.19 |\n",
      "0   8.11e+05   2248.04 |\n",
      "0   8.16e+05   2284.69 |\n",
      "0   8.17e+05   2284.69 | 2284.69     10.79       0.01      0.01\n",
      "0   8.30e+05   2284.69 | 2273.09      0.29       0.02      0.01\n",
      "0   8.45e+05   2284.69 | 1785.62    478.54       0.01      0.02\n",
      "0   8.47e+05   2293.12 |\n",
      "0   8.58e+05   2293.12 |  481.66      9.06       0.02      0.01\n",
      "0   8.71e+05   2293.12 | 1693.71    429.56       0.02      0.03\n",
      "0   8.73e+05   2306.52 |\n",
      "0   8.85e+05   2306.52 | 1735.90    540.53       0.02      0.02\n",
      "0   8.98e+05   2306.52 | 2246.32      4.56       0.02      0.02\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 5: Atari game (wait for adding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'breakout-v0'  # 'SpaceInvaders-v0'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
