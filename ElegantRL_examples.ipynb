{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: Discrete action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm\n",
    "from AgentZoo import AgentDoubleDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "from AgentRun import Arguments, train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| env_name: CartPole-v0, action space: if_discrete\n",
      "| state_dim: 4, action_dim: 2, action_max: 1, target_reward: 195.0\n"
     ]
    }
   ],
   "source": [
    "# environment\n",
    "from Env import decorate_env\n",
    "import gym  # gym of OpenAI is not necessary for ElegantRL (even RL)\n",
    "gym.logger.set_level(40)  # Block warning: 'WARN: Box bound precision lowered by casting to float32'\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env = decorate_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| GPU id: 0, cwd: ./AgentDoubleDQN/CartPole-v0_0\n",
      "| Remove history\n",
      "ID      Step      MaxR |    avgR      stdR       objA      objC\n",
      "0   0.00e+00    200.00 |\n",
      "ID      Step   TargetR |    avgR      stdR   UsedTime  ########\n",
      "0   1.02e+03    195.00 |  200.00      0.00         13  ########\n"
     ]
    }
   ],
   "source": [
    "# class for setting and hyper-parameters\n",
    "args = Arguments(agent_rl=AgentDoubleDQN, env=env, gpu_id=0)\n",
    "\n",
    "# set hyper-parameters\n",
    "args.break_step = int(1e3 * 8)  # UsedTime: 20s (reach target_reward 195)\n",
    "args.net_dim = 2 ** 7\n",
    "\n",
    "# train the agent\n",
    "train_and_evaluate(args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| env_name: LunarLander-v2, action space: if_discrete\n",
      "| state_dim: 8, action_dim: 4, action_max: 1, target_reward: 200\n",
      "| GPU id: 0, cwd: ./AgentDoubleDQN/CartPole-v0_0\n",
      "| Remove history\n",
      "ID      Step      MaxR |    avgR      stdR       objA      objC\n",
      "0   0.00e+00   -229.44 |\n",
      "0   1.02e+03   -157.52 |\n",
      "0   5.12e+03   -114.14 |\n",
      "0   6.14e+03    -51.93 |\n",
      "0   8.19e+03    106.39 |\n",
      "0   1.33e+04    215.38 |\n",
      "ID      Step   TargetR |    avgR      stdR   UsedTime  ########\n",
      "0   1.43e+04    200.00 |  215.38    103.27        194  ########\n"
     ]
    }
   ],
   "source": [
    "# train the agent in another env\n",
    "args.env = decorate_env(env=gym.make('LunarLander-v2'))\n",
    "args.net_dim = 2 ** 8  # change a default hyper-parameters\n",
    "args.break_step = int(1e5 * 8)  # UsedTime: 200s (reach target_reward 200)\n",
    "\n",
    "train_and_evaluate(args) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Continuous action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym  # don't worry about 'WARN: Box bound precision lowered by casting to float32'\n",
    "from AgentRun import Arguments, train_and_evaluate\n",
    "from AgentZoo import AgentTD3, AgentSAC, AgentPPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2.1: Off-policy TD3 and SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| env.spec.reward_threshold is None, so I set target_reward=-200\n",
      "| env_name: Pendulum-v0, action space: Continuous\n",
      "| state_dim: 3, action_dim: 1, action_max: 2.0, target_reward: -200\n",
      "| GPU id: j, cwd: ./AgentSAC/Pendulum-v0_j\n",
      "| Remove history\n",
      "ID      Step      MaxR |    avgR      stdR       objA      objC\n",
      "j   0.00e+00  -1340.63 |\n",
      "j   1.02e+03   -970.71 |\n",
      "j   4.10e+03   -817.50 |\n",
      "j   5.12e+03   -357.35 |\n",
      "j   6.14e+03   -259.11 |\n",
      "j   7.17e+03   -228.45 |\n",
      "j   8.19e+03   -158.32 |\n",
      "ID      Step   TargetR |    avgR      stdR   UsedTime  ########\n",
      "j   9.22e+03   -200.00 | -158.32    131.10        155  ########\n"
     ]
    }
   ],
   "source": [
    "args = Arguments(if_on_policy=False)  # if_on_policy=False in default\n",
    "\n",
    "# choose DRL algorithm (off-policy)\n",
    "args.agent_rl = AgentSAC  # AgentTD3\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.target_reward = -200  # set target_reward manually for env 'Pendulum-v0'\n",
    "args.env = decorate_env(env)\n",
    "args.reward_scale = 2 ** -3\n",
    "# args.env = decorate_env(env=gym.make('LunarLanderContinuous-v2'))\n",
    "# args.reward_scale = 2 ** -2\n",
    "# args.env = decorate_env(env=gym.make('BipedalWalker-v3'))  # recommend args.gamma = 0.95\n",
    "# args.gamma = 0.96\n",
    "\n",
    "train_and_evaluate(args) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2.2: On-policy PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| env.spec.reward_threshold is None, so I set target_reward=-200\n",
      "| env_name: Pendulum-v0, action space: Continuous\n",
      "| state_dim: 3, action_dim: 1, action_max: 2.0, target_reward: -200\n",
      "| GPU id: j, cwd: ./AgentPPO/Pendulum-v0_j\n",
      "| Remove history\n",
      "ID      Step      MaxR |    avgR      stdR       objA      objC\n",
      "j   0.00e+00  -1375.25 |\n",
      "j   9.60e+03  -1216.81 |\n",
      "j   1.28e+04  -1196.24 |\n",
      "j   1.60e+04  -1129.99 |\n",
      "j   4.80e+04  -1051.96 |\n",
      "j   8.32e+04   -999.29 |\n",
      "j   8.64e+04   -876.76 |\n",
      "j   9.92e+04   -797.66 |\n",
      "j   1.06e+05   -737.22 |\n",
      "j   1.12e+05   -732.56 |\n",
      "j   1.15e+05   -673.28 |\n",
      "j   1.22e+05   -649.79 |\n",
      "j   1.28e+05   -606.30 |\n",
      "j   1.73e+05   -562.12 |\n",
      "j   1.76e+05   -559.34 |\n",
      "j   1.79e+05   -552.79 |\n",
      "j   1.82e+05   -468.47 |\n",
      "j   1.86e+05   -401.66 |\n",
      "j   1.89e+05   -153.41 |\n",
      "ID      Step   TargetR |    avgR      stdR   UsedTime  ########\n",
      "j   1.92e+05   -200.00 | -153.41    101.49        228  ########\n"
     ]
    }
   ],
   "source": [
    "args = Arguments(if_on_policy=True)  # on-policy has different hyper-parameters from off-policy\n",
    "\n",
    "# choose DRL algorithm (off-policy)\n",
    "args.agent_rl = AgentPPO\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.target_reward = -200  # set target_reward manually for env 'Pendulum-v0'\n",
    "args.env = decorate_env(env)\n",
    "args.reward_scale = 2 ** -3\n",
    "# args.env = decorate_env(env=gym.make('LunarLanderContinuous-v2'))\n",
    "# args.reward_scale = 2 ** -2\n",
    "# args.env = decorate_env(env=gym.make('BipedalWalker-v3'))  # recommend args.gamma = 0.95\n",
    "# args.gamma = 0.96\n",
    "\n",
    "train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 3: FinanceMultiStockEnv from FinRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AgentEnv import FinanceMultiStockEnv\n",
    "from AgentRun import Arguments, train_and_evaluate\n",
    "from AgentZoo import AgentTD3, AgentSAC, AgentPPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments(if_on_policy=True)\n",
    "args.agent_rl = AgentPPO\n",
    "args.env = FinanceMultiStockEnv()  # a standard env for ElegantRL, not need decorate_env()\n",
    "args.env.target_reward = 10\n",
    "\n",
    "args.break_step = int(5e6 * 4) \n",
    "args.net_dim = 2 ** 8\n",
    "args.max_step = 1699\n",
    "args.max_memo = (args.max_step - 1) * 16\n",
    "args.batch_size = 2 ** 11\n",
    "args.repeat_times = 2 ** 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| GPU id: j, cwd: ./AgentPPO/FinanceStock-v1_j\n",
      "| Remove history\n",
      "ID      Step      MaxR |    avgR      stdR       objA      objC\n",
      "j   0.00e+00      1.96 |\n",
      "j   2.55e+04      2.08 |\n",
      "j   5.09e+04      2.16 |\n",
      "j   7.64e+04      2.26 |\n",
      "j   1.02e+05      2.36 |\n",
      "j   1.27e+05      2.48 |\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 4: PyBullet (MuJoCo) (wait for adding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym  # don't worry about 'WARN: Box bound precision lowered by casting to float32'\n",
    "import pybullet_envs  # PyBullet is free, but MuJoCo is paid\n",
    "from AgentRun import Arguments, train_and_evaluate\n",
    "from AgentZoo import AgentTD3, AgentSAC, AgentPPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'AntBulletEnv-v0'\n",
    "assert env_name in {\n",
    "    \"AntBulletEnv-v0\", \n",
    "    \"Walker2DBulletEnv-v0\", \n",
    "    \"HalfCheetahBulletEnv-v0\",\n",
    "    \"HumanoidBulletEnv-v0\", \n",
    "    \"HumanoidFlagrunBulletEnv-v0\", \n",
    "    \"HumanoidFlagrunHarderBulletEnv-v0\",\n",
    "}\n",
    "env = gym.make(env_name)\n",
    "env = decorate_env(env, if_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments()\n",
    "args.agnent_rl = AgentSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 5: Atari game (wait for adding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'breakout-v0'  # 'SpaceInvaders-v0'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
